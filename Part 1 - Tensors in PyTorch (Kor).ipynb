{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch를 활용한 딥러닝 입문\n",
    "\n",
    "이 노트북에서는 신경망을 구축하고 학습시키는 프레임워크인 [PyTorch](http://pytorch.org/)를 소개합니다. PyTorch는 여러 면에서 우리가 익숙한 Numpy 배열처럼 동작합니다. 사실, Numpy 배열 자체가 텐서(tensor)이며, PyTorch는 이러한 텐서를 GPU로 쉽게 이동시켜 신경망 학습에 필요한 빠른 처리를 가능하게 합니다.  \n",
    "\n",
    "또한 PyTorch는 **자동 미분**(즉, 역전파를 위한 기울기 계산)을 수행하는 모듈과, 신경망을 구축하는 데 특화된 모듈을 제공합니다. 이러한 기능 덕분에 PyTorch는 TensorFlow 및 기타 프레임워크보다 **Python 및 Numpy/Scipy 스택과 더 자연스럽게 통합되는** 특징을 가지고 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 (Neural Networks)\n",
    "\n",
    "딥러닝은 **인공 신경망(Artificial Neural Networks)**을 기반으로 하며, 이 개념은 1950년대 후반부터 존재해 왔습니다. 신경망은 뉴런을 모방한 개별 요소로 구성되며, 일반적으로 **유닛(units)** 또는 단순히 **뉴런(neurons)**이라고 불립니다.  \n",
    "\n",
    "각 유닛은 여러 개의 **가중치(weighted inputs)**를 가지며, 이 입력들은 선형 결합(linear combination)으로 합산된 후 **활성화 함수(activation function)**를 거쳐 최종 출력을 생성합니다.  \n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "수학적으로는 다음과 같이 표현됩니다:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "벡터(vector)로 표현하면 **내적(dot product)** 또는 **내부 곱(inner product)**과 같습니다:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 (Tensors)\n",
    "\n",
    "신경망 연산은 기본적으로 *텐서(tensor)*를 활용한 선형 대수 연산의 집합입니다. **텐서는 행렬(matrix)의 일반화된 개념**입니다.  \n",
    "\n",
    "- **벡터(vector)**: 1차원 텐서  \n",
    "- **행렬(matrix)**: 2차원 텐서  \n",
    "- **3차원 이상의 배열**: 다차원 텐서 (예: RGB 컬러 이미지처럼 3개의 색상 채널을 가진 이미지)\n",
    "\n",
    "즉, 신경망에서 가장 기본적인 데이터 구조는 **텐서**이며, PyTorch는 (대부분의 딥러닝 프레임워크와 마찬가지로) 텐서를 중심으로 설계되어 있습니다.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "이제 기본 개념을 이해했으므로, PyTorch를 사용하여 간단한 신경망을 구축하는 방법을 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def activation(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "torch.manual_seed(7)\n",
    "features = torch.randn((1,5))\n",
    "weights = torch.randn_like(features)\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서는 간단한 신경망의 출력을 얻기 위해 사용할 데이터를 생성했습니다. 현재는 모두 무작위 값이지만, 앞으로는 정상적인 데이터를 사용하기 시작할 것입니다. 각 관련 코드 라인을 살펴보겠습니다.\n",
    "\n",
    "`features = torch.randn((1, 5))`  \n",
    "이 코드는 `(1, 5)` 형태의 텐서를 생성합니다. 즉, 하나의 행과 다섯 개의 열을 가진 텐서를 만들며, 값은 평균이 0이고 표준 편차가 1인 정규 분포를 따르는 랜덤 값들입니다.\n",
    "\n",
    "`weights = torch.randn_like(features)`  \n",
    "이 코드는 `features`와 동일한 형태를 가진 또 다른 텐서를 생성하며, 역시 정규 분포를 따르는 랜덤 값들을 포함합니다.\n",
    "\n",
    "마지막으로, `bias = torch.randn((1, 1))`  \n",
    "이 코드는 하나의 값을 가지는 텐서를 생성하며, 이 값 역시 정규 분포에서 무작위로 샘플링됩니다.\n",
    "\n",
    "PyTorch의 텐서는 Numpy 배열처럼 더하기, 곱하기, 빼기 등의 연산이 가능합니다. 일반적으로 PyTorch 텐서는 Numpy 배열과 거의 동일한 방식으로 사용할 수 있습니다. 하지만 PyTorch는 GPU 가속과 같은 추가적인 장점도 제공하는데, 이에 대해서는 나중에 다룰 예정입니다.  \n",
    "\n",
    "지금은 생성한 데이터를 사용하여 이 간단한 단일 계층 신경망의 출력을 계산해 보겠습니다.  \n",
    "\n",
    "> **연습 문제**:  \n",
    "> 주어진 입력 특성 `features`, 가중치 `weights`, 그리고 편향 `bias`를 사용하여 신경망의 출력을 계산하세요.  \n",
    "> Numpy와 마찬가지로 PyTorch에는 [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) 함수와 `.sum()` 메서드가 있어 합계를 구할 수 있습니다.  \n",
    "> 위에서 정의한 `activation` 함수를 활성화 함수로 사용하여 출력을 계산해 보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the output of this network using the weights and bias tensors\n",
    "\n",
    "y = activation(torch.sum(features * weights) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "곱셈과 합산을 같은 연산에서 수행하려면 행렬 곱셈을 사용할 수 있습니다. 일반적으로 행렬 곱셈을 사용하는 것이 더 효율적이며, 최신 라이브러리와 GPU 기반의 고성능 컴퓨팅을 활용하여 가속화할 수 있습니다.\n",
    "\n",
    "여기서는 특징(feature)과 가중치(weight)의 행렬 곱셈을 수행하려고 합니다. 이를 위해 [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) 또는 [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul)을 사용할 수 있습니다. `torch.matmul()`은 보다 복잡하지만 브로드캐스팅을 지원합니다. 하지만 현재 `features`와 `weights`를 그대로 사용하면 다음과 같은 오류가 발생합니다.\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "신경망을 구축할 때 이런 오류를 자주 보게 될 것입니다. 여기서 발생한 문제는 텐서의 크기가 행렬 곱셈을 수행하기에 적절하지 않기 때문입니다. 행렬 곱셈에서는 첫 번째 텐서의 열 개수와 두 번째 텐서의 행 개수가 일치해야 합니다. 그러나 현재 `features`와 `weights`의 크기는 동일한 `(1, 5)`이므로, `weights`의 크기를 변경해야 행렬 곱셈이 가능합니다.\n",
    "\n",
    "**참고:** 텐서의 크기를 확인하려면 `tensor.shape`을 사용하면 됩니다. 신경망을 구축할 때 이 방법을 자주 사용하게 될 것입니다.\n",
    "\n",
    "이를 해결하는 방법에는 [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) 등이 있습니다.\n",
    "\n",
    "* `weights.reshape(a, b)`는 `weights`와 동일한 데이터를 가지며 크기가 `(a, b)`인 새로운 텐서를 반환합니다. 경우에 따라 원본 데이터를 복사할 수도 있습니다.\n",
    "* `weights.resize_(a, b)`는 같은 텐서를 다른 형태로 변경합니다. 하지만 새로운 크기가 원래보다 작으면 일부 요소가 제거되고, 더 크면 초기화되지 않은 요소가 메모리에 남아 있을 수 있습니다. 메서드 이름 끝의 밑줄(`_`)은 **해당 연산이 텐서에 직접 적용(in-place)됨**을 의미합니다. PyTorch의 in-place 연산에 대한 자세한 내용은 [이 포럼 글](https://discuss.pytorch.org/t/what-is-in-place-operation/16244)을 참고하세요.\n",
    "* `weights.view(a, b)`는 `weights`와 동일한 데이터를 가지며 크기가 `(a, b)`인 새로운 텐서를 반환합니다.\n",
    "\n",
    "보통 저는 `.view()`를 사용하지만, 세 가지 방법 모두 동일한 기능을 수행할 수 있습니다. 따라서 `weights`를 5개의 행과 1개의 열을 가지도록 변형하려면 `weights.view(5, 1)`을 사용할 수 있습니다.\n",
    "\n",
    "> **연습 문제:** 행렬 곱셈을 사용하여 신경망의 출력을 계산해 보세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the output of this network using matrix multiplication\n",
    "\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쌓아 올리세요!\n",
    "\n",
    "이것이 단일 뉴런의 출력을 계산하는 방법입니다. 하지만 이 알고리즘의 진정한 힘은 이러한 개별 유닛들을 층(layer)으로 쌓고, 여러 층을 쌓아 뉴런 네트워크를 만들 때 발휘됩니다. 한 층의 뉴런 출력이 다음 층의 입력이 됩니다. 다수의 입력 유닛과 출력 유닛이 있을 때, 이제 가중치를 행렬로 표현해야 합니다.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "맨 아래층은 입력값을 나타내며, **입력층(input layer)**이라고 합니다. 가운데층은 **은닉층(hidden layer)**이고, 맨 오른쪽층은 **출력층(output layer)**입니다. 이 네트워크를 수학적으로 행렬을 이용해 표현할 수 있으며, 행렬 곱셈을 사용하여 한 번의 연산으로 각 유닛의 선형 결합을 구할 수 있습니다. 예를 들어, 은닉층($h_1$과 $h_2$)을 다음과 같이 계산할 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "이 작은 네트워크의 출력을 구하려면, 은닉층을 출력 유닛의 입력으로 사용하면 됩니다. 이 네트워크의 출력은 간단히 다음과 같이 표현할 수 있습니다.\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 생성 및 랜덤 시드 설정\n",
    "torch.manual_seed(7)  # 난수 생성기의 시드를 고정하여 동일한 결과를 얻을 수 있도록 설정\n",
    "\n",
    "# 입력 데이터(features) 생성: 평균 0, 표준편차 1을 따르는 정규 분포를 따르는 난수 생성\n",
    "features = torch.randn((1, 3))  # 1개의 샘플(sample), 3개의 특성(feature)을 가진 텐서\n",
    "\n",
    "### 신경망의 계층 크기 정의\n",
    "n_input = features.shape[1]  # 입력층 뉴런 수 (특성 개수와 동일)\n",
    "n_hidden = 2                 # 은닉층(hidden layer) 뉴런 수 (사용자가 임의로 설정 가능)\n",
    "n_output = 1                 # 출력층(output layer) 뉴런 수 (회귀 문제라면 1, 다중 분류라면 클래스 수)\n",
    "\n",
    "### 가중치 및 편향(bias) 초기화\n",
    "# 입력층에서 은닉층으로 가는 가중치 행렬 (n_input x n_hidden 크기의 텐서)\n",
    "W1 = torch.randn(n_input, n_hidden)  # 입력 뉴런 개수 * 은닉층 뉴런 개수 크기의 랜덤 가중치 행렬\n",
    "\n",
    "# 은닉층에서 출력층으로 가는 가중치 행렬 (n_hidden x n_output 크기의 텐서)\n",
    "W2 = torch.randn(n_hidden, n_output)  # 은닉층 뉴런 개수 * 출력층 뉴런 개수 크기의 랜덤 가중치 행렬\n",
    "\n",
    "# 은닉층의 편향 벡터 (1 x n_hidden 크기의 텐서)\n",
    "B1 = torch.randn((1, n_hidden))  # 은닉층의 각 뉴런마다 하나의 편향값 존재\n",
    "\n",
    "# 출력층의 편향 벡터 (1 x n_output 크기의 텐서)\n",
    "B2 = torch.randn((1, n_output))  # 출력층의 각 뉴런마다 하나의 편향값 존재\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바르게 수행했다면 이와 `tensor([[ 0.3171]])`. 같은 값이 나올겁니다.\n",
    "\n",
    "은닉 유닛(hidden unit)의 개수는 네트워크의 매개변수 중 하나이며, 가중치(weight) 및 편향(bias)과 구분하기 위해 일반적으로 **하이퍼파라미터(hyperparameter)**라고 불립니다. 이후 신경망을 학습하는 과정에서 살펴보겠지만, 은닉 유닛의 개수가 많을수록, 그리고 층(layer)이 많을수록, 네트워크가 데이터를 더 잘 학습하고 보다 정확한 예측을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "\n",
    "Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06030457, 0.25649855, 0.9455632 ],\n",
       "       [0.88021366, 0.22169528, 0.06521363],\n",
       "       [0.9984028 , 0.43702998, 0.83918815],\n",
       "       [0.29774704, 0.08380211, 0.39461305]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=8)\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.06030457, 0.25649855, 0.94556320],\n",
       "        [0.88021366, 0.22169528, 0.06521363],\n",
       "        [0.99840280, 0.43702998, 0.83918815],\n",
       "        [0.29774704, 0.08380211, 0.39461305]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
