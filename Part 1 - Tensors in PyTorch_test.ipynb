{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch를 활용한 딥러닝 입문\n",
    "\n",
    "이 노트북에서는 신경망을 구축하고 학습시키는 프레임워크인 [PyTorch](http://pytorch.org/)를 소개합니다. PyTorch는 여러 면에서 우리가 익숙한 Numpy 배열처럼 동작합니다. 사실, Numpy 배열 자체가 텐서(tensor)이며, PyTorch는 이러한 텐서를 GPU로 쉽게 이동시켜 신경망 학습에 필요한 빠른 처리를 가능하게 합니다.  \n",
    "\n",
    "또한 PyTorch는 **자동 미분**(즉, 역전파를 위한 기울기 계산)을 수행하는 모듈과, 신경망을 구축하는 데 특화된 모듈을 제공합니다. 이러한 기능 덕분에 PyTorch는 TensorFlow 및 기타 프레임워크보다 **Python 및 Numpy/Scipy 스택과 더 자연스럽게 통합되는** 특징을 가지고 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 (Neural Networks)\n",
    "\n",
    "딥러닝은 **인공 신경망(Artificial Neural Networks)**을 기반으로 하며, 이 개념은 1950년대 후반부터 존재해 왔습니다. 신경망은 뉴런을 모방한 개별 요소로 구성되며, 일반적으로 **유닛(units)** 또는 단순히 **뉴런(neurons)**이라고 불립니다.  \n",
    "\n",
    "각 유닛은 여러 개의 **가중치(weighted inputs)**를 가지며, 이 입력들은 선형 결합(linear combination)으로 합산된 후 **활성화 함수(activation function)**를 거쳐 최종 출력을 생성합니다.  \n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "수학적으로는 다음과 같이 표현됩니다:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "벡터(vector)로 표현하면 **내적(dot product)** 또는 **내부 곱(inner product)**과 같습니다:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 (Tensors)\n",
    "\n",
    "신경망 연산은 기본적으로 *텐서(tensor)*를 활용한 선형 대수 연산의 집합입니다. **텐서는 행렬(matrix)의 일반화된 개념**입니다.  \n",
    "\n",
    "- **벡터(vector)**: 1차원 텐서  \n",
    "- **행렬(matrix)**: 2차원 텐서  \n",
    "- **3차원 이상의 배열**: 다차원 텐서 (예: RGB 컬러 이미지처럼 3개의 색상 채널을 가진 이미지)\n",
    "\n",
    "즉, 신경망에서 가장 기본적인 데이터 구조는 **텐서**이며, PyTorch는 (대부분의 딥러닝 프레임워크와 마찬가지로) 텐서를 중심으로 설계되어 있습니다.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "이제 기본 개념을 이해했으므로, PyTorch를 사용하여 간단한 신경망을 구축하는 방법을 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def activation(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "torch.manual_seed(7)\n",
    "features = torch.randn((1,5))\n",
    "weight = torch.randn_like(features)\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서는 간단한 신경망의 출력을 얻기 위해 사용할 데이터를 생성했습니다. 현재는 모두 무작위 값이지만, 앞으로는 정상적인 데이터를 사용하기 시작할 것입니다. 각 관련 코드 라인을 살펴보겠습니다.\n",
    "\n",
    "`features = torch.randn((1, 5))`  \n",
    "이 코드는 `(1, 5)` 형태의 텐서를 생성합니다. 즉, 하나의 행과 다섯 개의 열을 가진 텐서를 만들며, 값은 평균이 0이고 표준 편차가 1인 정규 분포를 따르는 랜덤 값들입니다.\n",
    "\n",
    "`weights = torch.randn_like(features)`  \n",
    "이 코드는 `features`와 동일한 형태를 가진 또 다른 텐서를 생성하며, 역시 정규 분포를 따르는 랜덤 값들을 포함합니다.\n",
    "\n",
    "마지막으로, `bias = torch.randn((1, 1))`  \n",
    "이 코드는 하나의 값을 가지는 텐서를 생성하며, 이 값 역시 정규 분포에서 무작위로 샘플링됩니다.\n",
    "\n",
    "PyTorch의 텐서는 Numpy 배열처럼 더하기, 곱하기, 빼기 등의 연산이 가능합니다. 일반적으로 PyTorch 텐서는 Numpy 배열과 거의 동일한 방식으로 사용할 수 있습니다. 하지만 PyTorch는 GPU 가속과 같은 추가적인 장점도 제공하는데, 이에 대해서는 나중에 다룰 예정입니다.  \n",
    "\n",
    "지금은 생성한 데이터를 사용하여 이 간단한 단일 계층 신경망의 출력을 계산해 보겠습니다.  \n",
    "\n",
    "> **연습 문제**:  \n",
    "> 주어진 입력 특성 `features`, 가중치 `weights`, 그리고 편향 `bias`를 사용하여 신경망의 출력을 계산하세요.  \n",
    "> Numpy와 마찬가지로 PyTorch에는 [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) 함수와 `.sum()` 메서드가 있어 합계를 구할 수 있습니다.  \n",
    "> 위에서 정의한 `activation` 함수를 활성화 함수로 사용하여 출력을 계산해 보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the output of this network using the weights and bias tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the multiplication and sum in the same operation using a matrix multiplication. In general, you'll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs.\n",
    "\n",
    "Here, we want to do a matrix multiplication of the features and the weights. For this we can use [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) which is somewhat more complicated and supports broadcasting. If we try to do it with `features` and `weights` as they are, we'll get an error\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "As you're building neural networks in any framework, you'll see this often. Really often. What's happening here is our tensors aren't the correct shapes to perform a matrix multiplication. Remember that for matrix multiplications, the number of columns in the first tensor must equal to the number of rows in the second column. Both `features` and `weights` have the same shape, `(1, 5)`. This means we need to change the shape of `weights` to get the matrix multiplication to work.\n",
    "\n",
    "**Note:** To see the shape of a tensor called `tensor`, use `tensor.shape`. If you're building neural networks, you'll be using this method often.\n",
    "\n",
    "There are a few options here: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
    "\n",
    "I usually use `.view()`, but any of the three methods will work for this. So, now we can reshape `weights` to have five rows and one column with something like `weights.view(5, 1)`.\n",
    "\n",
    "> **Exercise**: Calculate the output of our little network using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the output of this network using matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack them up!\n",
    "\n",
    "That's how you can calculate the output for a single neuron. The real power of this algorithm happens when you start stacking these individual units into layers and stacks of layers, into a network of neurons. The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "The first layer shown on the bottom here are the inputs, understandably called the **input layer**. The middle layer is called the **hidden layer**, and the final layer (on the right) is the **output layer**. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did this correctly, you should see the output `tensor([[ 0.3171]])`.\n",
    "\n",
    "The number of hidden units is a parameter of the network, often called a **hyperparameter** to differentiate it from the weights and biases parameters. As you'll see later when we discuss training a neural network, the more hidden units a network has, and the more layers, the better able it is to learn from data and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "\n",
    "Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=8)\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
